**Model Card — Super Tennis XGBoost (v2)**

Overview:
- **Model:** super_tennis_xgb_v2.json (XGBoost classifier)
- **Purpose:** Predict match winner (Player 1 vs Player 2) using engineered player and match-level features. Intended for analytics, pre-match projections, and model demonstration purposes.

Provenance & Authenticity:
- **Repository commit:** 4d39d40
- **Commit date:** 2025-12-14 11:35:09 +0530
- **Model artifact:** models/super_tennis_xgb_v2.json
- **SHA256 (model):** 8a5859cfd2462f3806e2568d1d2797715247fbd48407f25e231186f5712e6a42
- **Evaluation snapshot:** results/metrics.json (see section below)

Training Data & Features:
- **Training data (private):** Full dataset `data/n.csv` and `data/ml_ready5.csv`. The full dataset is not published for privacy and size reasons. A representative anonymized sample is included as `data/sample.csv` and described in `data/README.md`.
- **Feature engineering:** Derived features are generated by `src/m1.py` / `mc.py` (`add_features` function). Main features include pairwise/per-player statistics such as First/Second serve Win %, Return Win %, Fatigue, Clutch %, Momentum, Tiebreak Wins, Surface Strength, Break Conversion %, plus H2H strength and cluster labels (25 features total). See `src/m1.py` for exact feature names and the code used to compute them.

Training Procedure & Hyperparameters:
- **Algorithm:** XGBoost classifier (scikit-learn wrapper)
- **Key hyperparameters:** (as used in training) max_depth=5, learning_rate=0.05, n_estimators=300, subsample=0.8, random_state=42. (See `src/m1.py::train_model` for exact settings and training flow.)
- **Random seed & determinism:** `random_state=42` used to make training reproducible in our environment. Results may vary slightly across platforms and library versions.

Evaluation (sample reproduction):
- **Sample used for public evaluation:** `data/sample.csv` (n = 195 rows with labeled outcomes)
- **Reproduced metrics (from `results/metrics.json`):**
  - Accuracy: 0.51795
  - ROC AUC: 0.55468
  - n: 195
- **How we evaluated:** See `scripts/evaluate_sample.py`. It prepares derived features (mirroring `add_features`) on the sample, loads the model (`models/super_tennis_xgb_v2.json`), and computes accuracy, ROC AUC and plots (`results/roc.png`, `results/confusion.png`). You can reproduce locally by running `python scripts/evaluate_sample.py` after activating the project venv.

Reproducibility & Verification Steps (for clients):
- Clone repo and verify commit tag: `git rev-parse --short HEAD` should match the commit in this card.
- Verify model checksum:
  - `Get-FileHash models/super_tennis_xgb_v2.json -Algorithm SHA256` (Windows PowerShell)
  - Compare output to the **SHA256** above.
- Re-run public evaluation locally:
  - `python -m venv venv`
  - `venv\Scripts\python -m pip install -r requirements.txt`
  - `python scripts/evaluate_sample.py`  # recreates `results/` with metrics and plots
- Verify data sample integrity: `venv\Scripts\python scripts/verify_data.py` (checks `data/manifest.csv` hashes)

Limitations, Caveats & Responsible Use:
- **Sample vs full data:** The public sample is representative but small; metrics on the sample may not reflect final production performance.
- **Bias & fairness:** The model is trained on historical match results and may reflect biases present in the data (surfaces, tournament levels, regional biases). Please review the data access workflow in `DATA_ACCESS.md` before using the full dataset for downstream decisions.
- **Intended use:** Research, demonstration, and non-critical forecasting. Not intended as a standalone decision-making system for betting or high-stakes automated actions.

Contact & Next steps:
- For full dataset access, provenance artifacts, or re-training under an NDA, see `DATA_ACCESS.md` and follow the request process.
- If you want, I can add an automated `scripts/retrain.py` harness and CI job to fully reproduce training and push new model artifacts on demand — say the word and I’ll add it.

Appendix: Files to check for quick verification
- `scripts/evaluate_sample.py` — reproduces evaluation metrics and plots
- `scripts/make_sample.py` — creates anonymized sample (used to create `data/sample.csv`)
- `scripts/verify_data.py` — verifies sample and private data manifest checksums
- `src/m1.py`, `mc.py` — feature engineering and training logic
